{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4249f404",
   "metadata": {},
   "source": [
    "# Structured Output Generation Workshop\n",
    "\n",
    "In this workshop, we'll explore different techniques for generating structured output using Large Language Models (LLMs).\n",
    "We'll cover:\n",
    "1. Basic text to JSON conversion\n",
    "2. Working with different LLM providers\n",
    "3. Using Pydantic for type-safe parsing\n",
    "4. Understanding token probabilities\n",
    "5. Custom logits processing\n",
    "6. Structured generation with Outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f2fa4",
   "metadata": {},
   "source": [
    "## 1. Setting up our data sources\n",
    "First, let's create a helper function to fetch and cache our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_text(url: str, cache_file: str) -> str:\n",
    "    try:\n",
    "        with open(cache_file) as file:\n",
    "            text = file.read()\n",
    "    except FileNotFoundError:\n",
    "        response = requests.get(url)\n",
    "        soup = bs4.BeautifulSoup(response.text)\n",
    "        with open(cache_file, \"w\") as file:\n",
    "            file.write(soup.get_text())\n",
    "    return text\n",
    "\n",
    "\n",
    "planets_txt = get_text(\n",
    "    \"https://nssdc.gsfc.nasa.gov/planetary/factsheet/\",\n",
    "    \"data/planets.txt\",\n",
    ")\n",
    "satellites_txt = get_text(\n",
    "    \"https://ssd.jpl.nasa.gov/sats/phys_par/\",\n",
    "    \"data/satellites.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4542abb2",
   "metadata": {},
   "source": [
    "Let's examine our raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Planets data ===\")\n",
    "print(planets_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125cb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Satellites data ===\")\n",
    "print(satellites_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d090cb3",
   "metadata": {},
   "source": [
    "## 2. Basic Text to JSON Conversion\n",
    "Let's try converting our unstructured text data into JSON using an LLM.\n",
    "First, we'll set up our LLM client:\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5af582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# We'll start with Groq's API\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\", api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7836d6d4",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Try to write a prompt that would convert the planets data into JSON format.\n",
    "What challenges do you expect to face?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = ...  # fill this in\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": PROMPT},\n",
    "        {\"role\": \"user\", \"content\": planets_txt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "response_content = response.choices[0].message.content\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fde472",
   "metadata": {},
   "source": [
    "Let's validate if we got valid JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9960dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def is_json(maybe_json: str):\n",
    "    try:\n",
    "        json.loads(maybe_json)\n",
    "    except json.JSONDecodeError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "print(f\"Is valid JSON? {is_json(response_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2fb306",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Exercise 2\n",
    "How reliable is JSON conversion with different prompts? Let's experiment!\n",
    "\n",
    "Try writing different prompts and test their reliability. Here's a function to help you evaluate your prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20353022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt_reliability(prompt: str, n_trials: int = 10) -> float:\n",
    "    success = 0\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": planets_txt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        if is_json(response.choices[0].message.content):\n",
    "            success += 1\n",
    "\n",
    "    return success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073cead0",
   "metadata": {},
   "source": [
    "Let's test it with a basic prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5164ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Convert the following into a JSON format:\"\n",
    "success = test_prompt_reliability(PROMPT, n_trials=1)\n",
    "\n",
    "print(f\"Conversion successful: {success == 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397746a",
   "metadata": {},
   "source": [
    "Try different prompts! Here are some ideas to start with:\n",
    "- A simple \"Convert to JSON\" prompt\n",
    "- A detailed prompt with specific formatting instructions\n",
    "- A prompt that includes an example\n",
    "- A prompt that focuses on data types\n",
    "\n",
    "What success rates do you get? Why do you think some prompts work better than others?\n",
    "\n",
    "⚠️ Note: You might notice that even with your best prompt, getting consistent results is challenging.\n",
    "This will lead us to explore more reliable techniques in the next sections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_PROMPT = \"...\"  # Fill in your prompt here\n",
    "N_TRIALS = 10\n",
    "\n",
    "success = test_prompt_reliability(MY_PROMPT, N_TRIALS)\n",
    "print(f\"Success rate: {success} / {N_TRIALS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e645fe59",
   "metadata": {},
   "source": [
    "## 3. Using Built-in JSON Response Format\n",
    "Some LLM APIs provide built-in JSON formatting. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8423fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Convert the following into a JSON format:\"},\n",
    "        {\"role\": \"user\", \"content\": planets_txt},\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "\n",
    "response_content = response.choices[0].message.content\n",
    "print(response_content)\n",
    "\n",
    "print(f\"Is valid JSON? {is_json(response_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f31d2a3",
   "metadata": {},
   "source": [
    "This sometimes fails due to a flaw in the LLM output. We need another method we can trust..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb614640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import BadRequestError\n",
    "\n",
    "n_trials = 10\n",
    "success = 0\n",
    "for _ in range(n_trials):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Convert the following into a JSON format:\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": planets_txt},\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        success += 1\n",
    "    except BadRequestError as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Success rate: {success} / {n_trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87540a06",
   "metadata": {},
   "source": [
    "## 4. Type-Safe Parsing with Pydantic\n",
    "Let's make our data more structured and type-safe using Pydantic models.\n",
    "\n",
    "In our previous JSON conversions, did you notice any issues with the data types?\n",
    "For example, fields like `has_global_magnetic_field` and `surface_pressure` were getting converted to strings\n",
    "because in the source text they appeared as \"Yes\", \"No\", or \"Unknown\".\n",
    "\n",
    "This is a common problem when dealing with unstructured data. We want to:\n",
    "- Convert \"Yes\"/\"No\" to proper boolean values\n",
    "- Handle \"Unknown\" cases with None\n",
    "- Ensure numeric values are actually numbers, not strings\n",
    "- Make the schema explicit and reusable\n",
    "\n",
    "Pydantic helps us solve these issues by:\n",
    "1. Defining expected types for each field\n",
    "2. Handling type conversion automatically\n",
    "3. Supporting optional values with `| None` syntax\n",
    "4. Validating the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aafa34",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Planet(BaseModel):\n",
    "    name: str\n",
    "    mass: float\n",
    "    diameter: float\n",
    "    density: float\n",
    "    gravity: float\n",
    "    escape_velocity: float\n",
    "    rotation_period: float\n",
    "    length_of_day: float\n",
    "    distance_from_sun: float\n",
    "    perihelion: float\n",
    "    aphelion: float\n",
    "    orbital_period: float\n",
    "    orbital_velocity: float\n",
    "    orbital_inclination: float\n",
    "    orbital_eccentricity: float\n",
    "    obliquity_to_orbit: float\n",
    "    mean_temperature: float\n",
    "    surface_pressure: float | None\n",
    "    number_of_moons: int\n",
    "    has_ring_system: str\n",
    "    has_global_magnetic_field: bool | None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6813c0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Notice how we use `float | None` and `bool | None` for fields that might be unknown.\n",
    "This tells Pydantic to:\n",
    "- Convert the field to float/bool when possible\n",
    "- Use None when the value is \"Unknown\" or invalid\n",
    "\n",
    "⚠️ Always inspect your parsed data! Don't assume the LLM correctly interpreted all fields.\n",
    "Compare the output with the source text to ensure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89834028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarSystem(BaseModel):\n",
    "    planets: list[Planet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switching to OpenAI's API for Pydantic support\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.openai.com/v1\", api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Convert the following into a JSON format:\"},\n",
    "        {\"role\": \"user\", \"content\": planets_txt},\n",
    "    ],\n",
    "    response_format=SolarSystem,\n",
    ")\n",
    "\n",
    "response_content = response.choices[0].message.content\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f844b5",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Try to define a Pydantic model for the satellites data.\n",
    "What fields would you include?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Satellite(BaseModel):\n",
    "    name: str\n",
    "    # fill in the rest of the fields\n",
    "\n",
    "\n",
    "class SatelliteSystem(BaseModel):\n",
    "    satellites: list[Satellite]\n",
    "\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Convert the following into a JSON format:\"},\n",
    "        {\"role\": \"user\", \"content\": satellites_txt},\n",
    "    ],\n",
    "    response_format=SatelliteSystem,\n",
    ")\n",
    "\n",
    "response_content = response.choices[0].message.content\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d315e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Now let's combine both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilkyWay(BaseModel):\n",
    "    planets: list[Planet]\n",
    "    satellites: list[Satellite]\n",
    "\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Convert the following into a JSON format:\"},\n",
    "        {\"role\": \"user\", \"content\": planets_txt},\n",
    "        {\"role\": \"user\", \"content\": satellites_txt},\n",
    "    ],\n",
    "    response_format=MilkyWay,\n",
    ")\n",
    "\n",
    "response_content = response.choices[0].message.content\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18566937",
   "metadata": {},
   "source": [
    "## 5. Understanding Token Probabilities\n",
    "Let's explore how the model makes decisions by looking at token probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f420897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define a function to plot the top token probabilities\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_token_probs(tokens: list[str], probs: list[float]):\n",
    "    # Print probabilities\n",
    "    print(\"Top tokens the model might generate:\")\n",
    "    for i, (prob, token) in enumerate(zip(probs, tokens), start=1):\n",
    "        print(f\"{i}.\\t{token!r}\\t({prob:.2%})\")\n",
    "\n",
    "    # Visualize probabilities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(range(len(tokens)), probs)\n",
    "    plt.xticks(\n",
    "        ticks=range(len(tokens)),\n",
    "        labels=[repr(token) for token in tokens],\n",
    "        rotation=45,\n",
    "        ha=\"right\",\n",
    "    )\n",
    "    plt.title(\"Top Token Probabilities\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xlabel(\"Token\")\n",
    "\n",
    "    # Add percentage labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            f\"{height:.2%}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe87ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we access the token probabilities from the model completion\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Convert the following into a JSON format:\"},\n",
    "        {\"role\": \"user\", \"content\": planets_txt},\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    top_logprobs=10,\n",
    ")\n",
    "\n",
    "# Extract tokens and probabilities\n",
    "tokens = [lp.token for lp in response.choices[0].logprobs.content[0].top_logprobs]\n",
    "probs = [10**lp.logprob for lp in response.choices[0].logprobs.content[0].top_logprobs]\n",
    "\n",
    "# Plot and print the top token probabilities\n",
    "plot_token_probs(tokens, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2a2d6",
   "metadata": {},
   "source": [
    "### Constraining the Output\n",
    "Sometimes we want more control over what tokens the model can generate.\n",
    "We'll explore this using a simple example: forcing the model to answer only \"yes\" or \"no\".\n",
    "\n",
    "To have more control, we will load a model and run inference locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's set up our model:\n",
    "\n",
    "MODEL_NAME = \"openai-community/gpt2-xl\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad68af",
   "metadata": {},
   "source": [
    "Let's see how the model responds normally to our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad567a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Is Pluto a planet?\"\n",
    "\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "import torch\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "answer = tokenizer.decode(outputs[0])[len(PROMPT) :].strip()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd7534",
   "metadata": {},
   "source": [
    "But is it consistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e5029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0])[len(PROMPT) :]\n",
    "    answer_fmt = answer.replace(\"\\n\", \" \").strip()\n",
    "    print(f\"Attempt {i+1}:\\t{answer_fmt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6926bbc",
   "metadata": {},
   "source": [
    "Not really... we need to find another way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2fe358",
   "metadata": {},
   "source": [
    "### Understanding Token Probabilities\n",
    "Before we constrain the output, let's look at what tokens the model considers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f972fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "last_token_logit = logits[:, -1, :]\n",
    "next_token_probs = torch.nn.functional.softmax(last_token_logit, dim=-1)\n",
    "\n",
    "k = 10\n",
    "top_k_probs, top_k_indices = torch.topk(next_token_probs, k, dim=-1)\n",
    "top_k_tokens = [\n",
    "    tokenizer.decode(idx, skip_special_tokens=True) for idx in top_k_indices[0]\n",
    "]\n",
    "\n",
    "plot_token_probs(top_k_tokens, top_k_probs[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f4784",
   "metadata": {},
   "source": [
    "### Constraining the Output\n",
    "Now, let's use a LogitsProcessor to force the model to choose between \"yes\" and \"no\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d105c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "\n",
    "\n",
    "class YesNoLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"Forces the model to output either 'yes' or 'no'.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, initial_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.initial_length = initial_length\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        # If we already generated a response, mask everything\n",
    "        if len(input_ids[0]) > self.initial_length:\n",
    "            scores.fill_(-float(\"inf\"))\n",
    "            return scores\n",
    "\n",
    "        # Get the token IDs for \"yes\" and \"no\" from the tokenizer vocabulary\n",
    "        yes_token_id = self.tokenizer.encode(\"yes\", add_special_tokens=False)\n",
    "        no_token_id = self.tokenizer.encode(\"no\", add_special_tokens=False)\n",
    "\n",
    "        print(f\"{yes_token_id=}\")\n",
    "        print(f\"{no_token_id=}\")\n",
    "\n",
    "        # Access the logits for the \"yes\" and \"no\" tokens from the model output\n",
    "        yes_no_logits = scores[:, [yes_token_id[0], no_token_id[0]]]\n",
    "        print(f\"{yes_no_logits=}\")\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        yes_no_probs = torch.nn.functional.softmax(yes_no_logits, dim=-1)\n",
    "        print(f\"{yes_no_probs=}\")\n",
    "\n",
    "        yes_prob = yes_no_probs[:, 0]\n",
    "        no_prob = yes_no_probs[:, 1]\n",
    "\n",
    "        # Set all scores to -inf\n",
    "        scores.fill_(-float(\"inf\"))\n",
    "\n",
    "        # Set the scores for \"yes\" and \"no\" tokens to the probabilities\n",
    "        scores[:, yes_token_id[0]] = torch.where(\n",
    "            yes_prob > no_prob,\n",
    "            input=torch.tensor(float(\"inf\")),\n",
    "            other=torch.tensor(-float(\"inf\")),\n",
    "        )\n",
    "        scores[:, no_token_id[0]] = torch.where(\n",
    "            yes_prob <= no_prob,\n",
    "            input=torch.tensor(float(\"inf\")),\n",
    "            other=torch.tensor(-float(\"inf\")),\n",
    "        )\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "# Run the constrained generation\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(model.device)\n",
    "prompt_tokens_length = len(inputs[0])\n",
    "\n",
    "logits_processor = YesNoLogitsProcessor(tokenizer, prompt_tokens_length)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    logits_processor=[logits_processor],\n",
    "    max_length=prompt_tokens_length + 1,  # generate 1 token only\n",
    ")\n",
    "\n",
    "output_token = outputs[0, prompt_tokens_length:]\n",
    "output_decoded = tokenizer.decode(output_token, skip_special_tokens=True)\n",
    "print(output_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a9982",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Now that you've seen how the YesNoLogitsProcessor works, how would you modify it for different binary choices?\n",
    "For example:\n",
    "- true/false responses\n",
    "- positive/negative sentiment\n",
    "- agree/disagree answers\n",
    "\n",
    "Try to sketch out the changes needed - what would you need to modify in the processor?\n",
    "\n",
    "Key points to consider:\n",
    "- How would you change the token IDs being used?\n",
    "- Would you need to modify the probability comparison?\n",
    "- What other modifications might be needed for your specific use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd006031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, initial_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.initial_length = initial_length\n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        # Your code here\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_PROMPT = \"...\"  # Fill in your prompt here\n",
    "\n",
    "# Run the constrained generation\n",
    "inputs = tokenizer(MY_PROMPT, return_tensors=\"pt\").to(model.device)\n",
    "prompt_tokens_length = len(inputs[0])\n",
    "\n",
    "logits_processor = YesNoLogitsProcessor(tokenizer, prompt_tokens_length)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    logits_processor=[logits_processor],\n",
    "    max_length=prompt_tokens_length + 1,  # generate 1 token only\n",
    ")\n",
    "\n",
    "output_token = outputs[0, prompt_tokens_length:]\n",
    "output_decoded = tokenizer.decode(output_token, skip_special_tokens=True)\n",
    "print(output_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14811e",
   "metadata": {},
   "source": [
    "## 7. Structured Generation with Outlines\n",
    "Finally, let's look at how the Outlines library makes structured generation easier.\n",
    "We'll explore three approaches to defining choices:\n",
    "1. Using a simple list of options\n",
    "2. Using an Enum for type-safe choices\n",
    "3. Using a more complex structure, a Pydantic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88858347",
   "metadata": {},
   "source": [
    "### 7.1 List-based Choices\n",
    "The simplest way to constrain outputs is with a list of allowed values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "\n",
    "outlines_model = outlines.models.transformers(MODEL_NAME)\n",
    "\n",
    "context = \"\"\"\n",
    "Saturn is the sixth planet from the Sun and is best known for its spectacular ring system, which is the most extensive of any planet in our solar system.\n",
    "The rings are made up of countless small particles of ice and rock, creating a stunning visual display.\n",
    "Other planets in our solar system, such as Jupiter, Uranus, and Neptune, also have ring systems, but none are as prominent or extensive as Saturn's.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Based on the following text, which of the following planets has the most extensive ring system?\n",
    "\n",
    "Text: {context}\n",
    "\"\"\"\n",
    "\n",
    "# Simple list of choices\n",
    "generator = outlines.generate.choice(\n",
    "    outlines_model, [\"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"]\n",
    ")\n",
    "answer = generator(prompt)\n",
    "print(f\"List-based choice: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40367a6c",
   "metadata": {},
   "source": [
    "### 7.2 Enum-based Choices\n",
    "For more complex applications, we can use Enums to:\n",
    "- Ensure type safety\n",
    "- Make choices self-documenting\n",
    "- Group related options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425cf15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class CelestialBody(str, Enum):\n",
    "    PLANET = \"planet\"\n",
    "    STAR = \"star\"\n",
    "    MOON = \"moon\"\n",
    "    ASTEROID = \"asteroid\"\n",
    "    COMET = \"comet\"\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "The Sun is a massive, luminous sphere of hot plasma that generates energy through nuclear fusion.\n",
    "It is the central object in our solar system and provides the necessary light and heat for life on Earth.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Based on the following text, what type of celestial body is being described?\n",
    "\n",
    "Text: {context}\n",
    "\"\"\"\n",
    "\n",
    "# Enum-based choices\n",
    "generator = outlines.generate.choice(outlines_model, CelestialBody)\n",
    "answer = generator(prompt)\n",
    "print(f\"Enum-based choice: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da554ec3",
   "metadata": {},
   "source": [
    "### 7.3 Using Pydantic models\n",
    "We can combine Enums and Pydantic models for even more structured output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9092c3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class AtmosphereType(str, Enum):\n",
    "    NONE = \"none\"\n",
    "    THIN = \"thin\"\n",
    "    THICK = \"thick\"\n",
    "    DENSE = \"dense\"\n",
    "\n",
    "\n",
    "class SurfaceType(str, Enum):\n",
    "    ROCKY = \"rocky\"\n",
    "    ICY = \"icy\"\n",
    "    GASEOUS = \"gaseous\"\n",
    "    METALLIC = \"metallic\"\n",
    "    VOLCANIC = \"volcanic\"\n",
    "\n",
    "\n",
    "class Atmosphere(BaseModel):\n",
    "    type: AtmosphereType\n",
    "    main_component: str\n",
    "    has_clouds: bool\n",
    "    pressure_bars: float\n",
    "\n",
    "\n",
    "class CelestialObject(BaseModel):\n",
    "    name: str\n",
    "    type: CelestialBody  # Using our previously defined CelestialBody enum\n",
    "    diameter_km: float\n",
    "    surface: SurfaceType\n",
    "    atmosphere: Atmosphere | None\n",
    "    number_of_satellites: int\n",
    "\n",
    "\n",
    "# Create a generator for this complex structure\n",
    "generator = outlines.generate.json(outlines_model, CelestialObject)\n",
    "\n",
    "context = \"\"\"\n",
    "Mars is the fourth planet from the Sun and the second-smallest planet (d = 6,779 km) in the Solar System, after Mercury.\n",
    "It is a rocky planet with a thin atmosphere composed mainly of carbon dioxide.\n",
    "Mars has two small moons, Phobos and Deimos, which are thought to be captured asteroids.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Based on the following text, convert the information into a structured format:\n",
    "\n",
    "Text: {context}\n",
    "\"\"\"\n",
    "\n",
    "celestial_object = generator(prompt)\n",
    "print(repr(celestial_object))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d8538",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Exercise 5\n",
    "Create your own structured generation example using Outlines.\n",
    "You can choose any of these approaches:\n",
    "\n",
    "1. Simple list-based choices:\n",
    "```python\n",
    "choices = [\"option1\", \"option2\", \"option3\"]\n",
    "generator = outlines.generate.choice(outlines_model, choices)\n",
    "```\n",
    "\n",
    "2. Enum-based choices:\n",
    "```python\n",
    "class YourChoices(str, Enum):\n",
    "    CHOICE1 = \"value1\"\n",
    "    CHOICE2 = \"value2\"\n",
    "generator = outlines.generate.choice(outlines_model, YourChoices)\n",
    "```\n",
    "\n",
    "3. Complex nested models:\n",
    "```python\n",
    "class SubModel(BaseModel):\n",
    "    field1: str\n",
    "    field2: YourChoices  # Using an Enum\n",
    "\n",
    "class MainModel(BaseModel):\n",
    "    name: str\n",
    "    sub_data: SubModel\n",
    "\n",
    "generator = outlines.generate.json(outlines_model, MainModel)\n",
    "```\n",
    "\n",
    "Think about which approach better suits your use case:\n",
    "- Use lists for simple, one-off choices\n",
    "- Use enums for reusable, type-safe choices\n",
    "- Use nested models for complex, structured data\n",
    "\n",
    "Find some unstructured data online for this. You can use the `get_text` function defined at the beginning of this notebook.\n",
    "\n",
    "Some ideas:\n",
    "- Astronomical object classification\n",
    "- Weather report structuring\n",
    "- Character/Species description\n",
    "- Scientific observation recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea138be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(BaseModel):\n",
    "    # Fill in the fields for your custom model\n",
    "\n",
    "\n",
    "# Create a generator for this complex structure\n",
    "\n",
    "generator = outlines.generate.json(outlines_model, MyModel)\n",
    "\n",
    "context = \"\"\"\n",
    "... # Fill in the context here, using get_text() or any other method\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Based on the following text, convert the information into a structured format:\n",
    "\n",
    "Text: {context}\n",
    "\"\"\"\n",
    "\n",
    "my_model = generator(prompt)\n",
    "print(repr(my_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441848e",
   "metadata": {},
   "source": [
    "## 6. Adding Data Validation for LLM Outputs\n",
    "\n",
    "One key challenge when working with LLMs is that they can:\n",
    "1. Generate physically impossible values\n",
    "2. Make mathematical errors\n",
    "3. Fail to maintain consistency between related values\n",
    "4. Hallucinate plausible-looking but incorrect data\n",
    "\n",
    "Even with perfect LLM output, the source data itself might contain errors or inconsistencies.\n",
    "Therefore, we need robust validation to:\n",
    "- Enforce physical constraints (e.g., positive mass, speed less than light)\n",
    "- Check mathematical relationships (e.g., orbital parameters)\n",
    "- Validate consistency between related values\n",
    "- Flag impossible combinations\n",
    "\n",
    "Let's look at some examples using astronomical data, where physical constraints\n",
    "are particularly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7319412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create some questionable astronomical texts for the LLM to process\n",
    "questionable_star_text = \"\"\"\n",
    "Alpha Centauri is a remarkable star system located -4.37 light years from Earth.\n",
    "The main star has a negative mass of -2.1 solar masses and a radius of 0 kilometers.\n",
    "Scientists believe it might be both a neutron star and a red giant simultaneously.\n",
    "\"\"\"\n",
    "\n",
    "impossible_planetary_system = \"\"\"\n",
    "The Kepler-X system contains a small star with mass 1e28 kg.\n",
    "It has three planets:\n",
    "1. Super-Jupiter: A massive planet with mass 1e29 kg (10 times more than its star!)\n",
    "2. Speedy: Completes an orbit in -2 days at a distance of 1e8 km\n",
    "3. Paradox: Has a closest approach (perihelion) of 2e8 km but furthest point (aphelion) of 1e8 km\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a33077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.openai.com/v1\", api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "# First, let's see what the LLM generates without validation\n",
    "class Star(BaseModel):\n",
    "    name: str\n",
    "    distance_ly: float\n",
    "    mass_solar: float\n",
    "    radius_km: float\n",
    "    type: str\n",
    "\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Convert the following star description into structured data:\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": questionable_star_text},\n",
    "    ],\n",
    "    response_format=Star,\n",
    ")\n",
    "\n",
    "print(\"Without validation:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df160ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's add validators\n",
    "\n",
    "from pydantic import ValidationError, ValidationInfo, field_validator, model_validator\n",
    "\n",
    "\n",
    "class ValidatedStar(BaseModel):\n",
    "    name: str\n",
    "    distance_ly: float\n",
    "    mass_solar: float\n",
    "    radius_km: float\n",
    "    type: str\n",
    "\n",
    "    @field_validator(\"distance_ly\", \"mass_solar\", \"radius_km\")\n",
    "    @classmethod\n",
    "    def must_be_positive(cls, value: float, info: ValidationInfo) -> float:\n",
    "        if value <= 0:\n",
    "            raise ValueError(f\"{info.field_name} must be positive\")\n",
    "        return value\n",
    "\n",
    "    @field_validator(\"type\")\n",
    "    @classmethod\n",
    "    def validate_star_type(cls, value: str) -> str:\n",
    "        valid_types = {\n",
    "            \"red dwarf\",\n",
    "            \"red giant\",\n",
    "            \"neutron star\",\n",
    "            \"white dwarf\",\n",
    "            \"main sequence\",\n",
    "        }\n",
    "        if value.lower() not in valid_types:\n",
    "            raise ValueError(f\"Invalid star type. Must be one of: {valid_types}\")\n",
    "        return value.lower()\n",
    "\n",
    "\n",
    "try:\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Convert the following star description into structured data:\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": questionable_star_text},\n",
    "        ],\n",
    "        response_format=ValidatedStar,\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(\"\\nWith validation:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b836da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's validate a planetary system\n",
    "\n",
    "from typing import Self\n",
    "\n",
    "\n",
    "class Planet(BaseModel):\n",
    "    name: str\n",
    "    mass_kg: float\n",
    "    orbital_period_days: float\n",
    "    perihelion_km: float\n",
    "    aphelion_km: float\n",
    "\n",
    "    @field_validator(\"mass_kg\", \"orbital_period_days\", \"perihelion_km\", \"aphelion_km\")\n",
    "    @classmethod\n",
    "    def must_be_positive(cls, value: float, info: ValidationInfo) -> float:\n",
    "        if value <= 0:\n",
    "            raise ValueError(f\"{info.field_name} must be positive\")\n",
    "        return value\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_orbit(self) -> Self:\n",
    "        if self.perihelion_km >= self.aphelion_km:\n",
    "            raise ValueError(\n",
    "                f\"Perihelion ({self.perihelion_km:e} km) must be less than \"\n",
    "                f\"aphelion ({self.aphelion_km:e} km)\"\n",
    "            )\n",
    "        return self\n",
    "\n",
    "\n",
    "class PlanetarySystem(BaseModel):\n",
    "    star_name: str\n",
    "    star_mass_kg: float\n",
    "    planets: list[Planet]\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_masses(self) -> Self:\n",
    "        for planet in self.planets:\n",
    "            if planet.mass_kg >= self.star_mass_kg:\n",
    "                raise ValueError(\n",
    "                    f\"Planet {planet.name} has mass {planet.mass_kg:e} kg, which is \"\n",
    "                    f\"greater than or equal to its star ({self.star_mass_kg:e} kg)\"\n",
    "                )\n",
    "        return self\n",
    "\n",
    "\n",
    "try:\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Convert this planetary system description into structured data:\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": impossible_planetary_system},\n",
    "        ],\n",
    "        response_format=PlanetarySystem,\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(\"\\nPlanetary system validation errors:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418f228",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Black Hole Validator\n",
    "Create a validator for black hole data. The LLM might generate physically impossible values.\n",
    "Key physics to check:\n",
    "- Event horizon radius (R) = 2GM/c² (G = gravitational constant, M = mass, c = speed of light)\n",
    "- Hawking radiation temperature ∝ 1/M\n",
    "- Singularity must be within event horizon\n",
    "\n",
    "Here's some intentionally problematic text to test with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab354d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "impossible_black_hole = \"\"\"\n",
    "BH-123 is a unique black hole with:\n",
    "- Mass: -5e30 kg (negative mass!)\n",
    "- Event horizon: 100 km (too large for its mass)\n",
    "- Singularity distance: 200 km (outside event horizon!)\n",
    "- Hawking temperature: -290 K (negative temperature!)\n",
    "\"\"\"\n",
    "\n",
    "G = 6.674e-11  # gravitational constant\n",
    "c = 3e8  # speed of light\n",
    "\n",
    "\n",
    "class BlackHole(BaseModel):\n",
    "    name: str\n",
    "    mass_kg: float\n",
    "    event_horizon_radius_km: float\n",
    "    singularity_distance_km: float | None = None\n",
    "    hawking_temperature_k: float | None = None\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_event_horizon(self) -> Self:\n",
    "        # calculate expected radius\n",
    "        expected_radius =  # fill in the formula here\n",
    "        if # fill in the condition here\n",
    "            raise ValueError(\n",
    "                f\"...\"  # fill in the error message here\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_singularity(self) -> Self:\n",
    "        if self.singularity_distance_km is not None:\n",
    "            # fill code here\n",
    "        return self\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_temperature(self) -> Self:\n",
    "        # fill code here\n",
    "\n",
    "\n",
    "try:\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Convert this black hole description into structured data:\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": impossible_black_hole},\n",
    "        ],\n",
    "        response_format=BlackHole,\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(\"\\nBlack hole validation errors:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2d945",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Testing with Valid Data\n",
    "\n",
    "Now that we've seen how our validators catch impossible values, let's try them with\n",
    "physically possible data. Create your own `possible_black_hole` text with realistic values.\n",
    "\n",
    "Some tips for realistic values:\n",
    "- Stellar black holes typically have masses of 3-100 solar masses (1 solar mass ≈ 2e30 kg)\n",
    "- The event horizon radius should follow r = 2GM/c²\n",
    "- Any singularity distance should be > 0 but < event horizon radius\n",
    "- Hawking temperature is inversely proportional to mass\n",
    "\n",
    "Try running your data through the validator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6063fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_black_hole = \"\"\"\n",
    "# Fill in your own realistic black hole description here\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Convert this black hole description into structured data:\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": possible_black_hole},\n",
    "        ],\n",
    "        response_format=BlackHole,\n",
    "    )\n",
    "    print(\"Validation successful!\")\n",
    "    print(response.choices[0].message.content)\n",
    "except ValidationError as e:\n",
    "    print(\"Validation failed:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee4935",
   "metadata": {},
   "source": [
    "## 8. Simple Vision Models with Structured Output\n",
    "Let's explore how to get structured output from vision models using Outlines.\n",
    "\n",
    "import outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028982ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlavaNextForConditionalGeneration\n",
    "\n",
    "# Initialize our model\n",
    "model = outlines.models.transformers_vision(\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    model_class=LlavaNextForConditionalGeneration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829baa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ImageData(BaseModel):\n",
    "    caption: str\n",
    "    tags_list: list[str]\n",
    "    object_list: list[str]\n",
    "    is_photo: bool\n",
    "\n",
    "# Create our structured generator\n",
    "image_data_generator = outlines.generate.json(model, ImageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc00ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def img_from_url(url: str) -> Image.Image:\n",
    "    \"\"\"Load an image from a URL and convert it to RGB format.\"\"\"\n",
    "    img_byte_stream = BytesIO(urlopen(url).read())\n",
    "    return Image.open(img_byte_stream).convert(\"RGB\")\n",
    "\n",
    "# Test with a famous image\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/9/98/Aldrin_Apollo_11_original.jpg\"\n",
    "image = img_from_url(image_url)\n",
    "\n",
    "# Lower image quality for faster processing\n",
    "image = image.resize((256, 256))\n",
    "\n",
    "# Generate structured output\n",
    "result = image_data_generator(\n",
    "    \"<image> detailed JSON metadata:\",\n",
    "    [image]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532facac",
   "metadata": {},
   "source": [
    "### Exercise: Testing with Different Images\n",
    "Try running the structured generation with different types of images to see how the model performs."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
